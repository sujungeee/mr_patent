import os
import sys
import pickle
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.tfidf_service import TfidfProcessor
from app.services.word2vec_service import Word2VecEnhancer
from app.core.config import settings
from app.db.session import SessionLocal
from app.models.patent import Patent

def train_and_save_models():
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŠ¹í—ˆ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ ëª¨ë¸ í•™ìŠµ ë° ì €ì¥"""
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(settings.MODEL_PATH, exist_ok=True)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    db = SessionLocal()
    
    try:
        # ëª¨ë“  íŠ¹í—ˆ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        patents = db.query(Patent).all()
        
        if not patents:
            print("âš ï¸ íŠ¹í—ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŠ¹í—ˆ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            return
        
        print(f"ğŸ”„ {len(patents)}ê°œì˜ íŠ¹í—ˆ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        contents = [patent.content for patent in patents]
        
        # TF-IDF ì²˜ë¦¬
        tfidf_processor = TfidfProcessor()
        processed_texts = []
        tokenized_docs = []
        
        for content in contents:
            processed_text, tokens = tfidf_processor.preprocess(content)
            processed_texts.append(processed_text)
            tokenized_docs.append(tokens)
        
        # TF-IDF ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
        print("ğŸ“Š TF-IDF ëª¨ë¸ í•™ìŠµ ì¤‘...")
        tfidf_matrix = tfidf_processor.fit_transform(processed_texts)
        
        # TF-IDF ëª¨ë¸ ì €ì¥
        with open(os.path.join(settings.MODEL_PATH, "tfidf_model.pkl"), 'wb') as f:
            pickle.dump(tfidf_processor.vectorizer, f)
        print("âœ… TF-IDF ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        
        # Word2Vec ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ (ì„¤ì •ì—ì„œ í™œì„±í™”ëœ ê²½ìš°)
        if settings.USE_WORD2VEC:
            print("ğŸ”¤ Word2Vec ëª¨ë¸ í•™ìŠµ ì¤‘...")
            word2vec_enhancer = Word2VecEnhancer()
            word2vec_model = word2vec_enhancer.train(tokenized_docs)
            
            # Word2Vec ëª¨ë¸ ì €ì¥
            word2vec_model.save(os.path.join(settings.MODEL_PATH, "word2vec_model.pkl"))
            print("âœ… Word2Vec ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
        
        print("ğŸ‰ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    finally:
        db.close()

if __name__ == "__main__":
    train_and_save_models()
