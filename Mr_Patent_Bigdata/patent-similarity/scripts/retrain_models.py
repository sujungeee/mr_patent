import os
import sys
import pickle
import numpy as np
import sqlite3
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from app.services.tfidf_service import TfidfProcessor
from app.services.word2vec_service import Word2VecEnhancer
from app.core.config import settings
from app.db.session import SessionLocal
from app.models.patent import Patent

def retrain_models_and_update_vectors():
    """ëª¨ë¸ ì¬í•™ìŠµ ë° íŠ¹í—ˆ ë°ì´í„° ë²¡í„° ì—…ë°ì´íŠ¸"""
    # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(settings.MODEL_PATH, exist_ok=True)
    
    # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
    db = SessionLocal()
    
    try:
        # ëª¨ë“  íŠ¹í—ˆ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        patents = db.query(Patent).all()
        
        if not patents:
            print("âš ï¸ íŠ¹í—ˆ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŠ¹í—ˆ ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            return
        
        print(f"ğŸ”„ {len(patents)}ê°œì˜ íŠ¹í—ˆ ë°ì´í„°ë¡œ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        # í…ìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        contents = [patent.content for patent in patents]
        
        # TF-IDF ì²˜ë¦¬
        tfidf_processor = TfidfProcessor()
        processed_texts = []
        tokenized_docs = []
        
        for content in contents:
            processed_text, tokens = tfidf_processor.preprocess(content)
            processed_texts.append(processed_text)
            tokenized_docs.append(tokens)
        
        # TF-IDF ëª¨ë¸ í•™ìŠµ ë° ì €ì¥
        print("ğŸ“Š TF-IDF ëª¨ë¸ í•™ìŠµ ì¤‘...")
        tfidf_matrix = tfidf_processor.fit_transform(processed_texts)
        
        # TF-IDF ëª¨ë¸ ì €ì¥
        tfidf_model_path = os.path.join(settings.MODEL_PATH, "tfidf_model.pkl")
        with open(tfidf_model_path, 'wb') as f:
            pickle.dump(tfidf_processor.vectorizer, f)
        print(f"âœ… TF-IDF ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {tfidf_model_path}")
        
        # Word2Vec ëª¨ë¸ í•™ìŠµ ë° ì €ì¥ (ì„¤ì •ì—ì„œ í™œì„±í™”ëœ ê²½ìš°)
        word2vec_enhancer = None
        if settings.USE_WORD2VEC:
            print("ğŸ”¤ Word2Vec ëª¨ë¸ í•™ìŠµ ì¤‘...")
            word2vec_enhancer = Word2VecEnhancer()
            word2vec_model = word2vec_enhancer.train(tokenized_docs)
            
            # Word2Vec ëª¨ë¸ ì €ì¥
            word2vec_model_path = os.path.join(settings.MODEL_PATH, "word2vec_model.pkl")
            word2vec_model.save(word2vec_model_path)
            print(f"âœ… Word2Vec ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {word2vec_model_path}")
        
        # ê¸°ì¡´ íŠ¹í—ˆ ë°ì´í„° ë²¡í„° ì—…ë°ì´íŠ¸
        print("ğŸ”„ ê¸°ì¡´ íŠ¹í—ˆ ë°ì´í„° ë²¡í„° ì—…ë°ì´íŠ¸ ì¤‘...")
        for i, patent in enumerate(patents):
            try:
                # ë²¡í„° ìƒì„±
                tfidf_vector = tfidf_processor.transform(processed_texts[i])
                
                # ë²¡í„° ì—…ë°ì´íŠ¸
                patent.tfidf_vector = pickle.dumps(tfidf_vector)
                
                # Word2Vec ë²¡í„° ì¶”ê°€ (ì‚¬ìš©í•˜ëŠ” ê²½ìš°)
                if word2vec_enhancer and settings.USE_WORD2VEC:
                    word2vec_vector = word2vec_enhancer.get_document_vector(tokenized_docs[i])
                    patent.word2vec_vector = pickle.dumps(word2vec_vector)
                
                # 100ê°œë§ˆë‹¤ ìƒíƒœ ì¶œë ¥
                if (i + 1) % 100 == 0 or i == len(patents) - 1:
                    print(f"â³ íŠ¹í—ˆ ë²¡í„° ì—…ë°ì´íŠ¸ ì§„í–‰ ì¤‘: {i+1}/{len(patents)}")
                    db.commit()  # ì¤‘ê°„ ì»¤ë°‹
            
            except Exception as e:
                print(f"âŒ íŠ¹í—ˆ ID {patent.id} ë²¡í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        
        # ìµœì¢… ì»¤ë°‹
        db.commit()
        print("ğŸ‰ ëª¨ë“  ëª¨ë¸ í•™ìŠµ ë° ë²¡í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        db.rollback()
    finally:
        db.close()

if __name__ == "__main__":
    retrain_models_and_update_vectors()
